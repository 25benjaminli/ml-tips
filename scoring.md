## Scoring methods
Precision = finding how well your model can classify positives. E.g., if you classify something as positive, what are the chances that it is ACTUALLY positive? 

Out of all positive CLASSIFICATIONS, how many are actually positive?

## Cares about how accurately you identify a positive - is there a consequence if you misclassify a positive as a negative?

-- If your model has low precision, then it is probably classifying a lot of negatives as positives

Recall = finding how well your model can find positives. E.g., if something is actually positive, what are the chances that your model will find it? 

Out of all positive samples, how many can the model classify properly?

## Basically ignores if you misclassify a negative as a positive though. Is there a consequence if you misclassify a negative as a positive

